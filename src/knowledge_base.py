# src/knowledge_base.py
"""
å‘é‡çŸ¥è¯†åº“ç®¡ç†
"""
import os
import json
from typing import List, Dict, Optional
from langchain_ollama import OllamaEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document
from dataclasses import asdict

from .config import config
from .paper_loader import PaperLoader, PaperMetadata


class ResearchKnowledgeBase:
    """ç§‘ç ”çŸ¥è¯†åº“"""

    def __init__(self):
        self.embeddings = OllamaEmbeddings(model=config.EMBEDDING_MODEL)
        self.paper_loader = PaperLoader()
        self.papers_index: Dict[str, PaperMetadata] = {}  # è®ºæ–‡ç´¢å¼•
        self.index_file = os.path.join(config.CHROMA_DIR, "papers_index.json")

        self._init_vectorstore()
        self._load_papers_index()

    def _init_vectorstore(self):
        """åˆå§‹åŒ–å‘é‡æ•°æ®åº“"""
        self.vectorstore = Chroma(
            persist_directory=config.CHROMA_DIR,
            embedding_function=self.embeddings,
            collection_name=config.COLLECTION_NAME
        )

    def _load_papers_index(self):
        """åŠ è½½è®ºæ–‡ç´¢å¼•"""
        if os.path.exists(self.index_file):
            with open(self.index_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                for k, v in data.items():
                    self.papers_index[k] = PaperMetadata(**v)
            print(f"ğŸ“š å·²åŠ è½½ {len(self.papers_index)} ç¯‡è®ºæ–‡ç´¢å¼•")

    def _save_papers_index(self):
        """ä¿å­˜è®ºæ–‡ç´¢å¼•"""
        data = {k: asdict(v) for k, v in self.papers_index.items()}
        with open(self.index_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def add_paper(self, file_path: str) -> PaperMetadata:
        """æ·»åŠ è®ºæ–‡åˆ°çŸ¥è¯†åº“"""
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if file_path in self.papers_index:
            print(f"âš ï¸ è®ºæ–‡å·²å­˜åœ¨: {file_path}")
            return self.papers_index[file_path]

        # åŠ è½½å¹¶è§£æ
        chunks, metadata = self.paper_loader.load_pdf(file_path)

        # æ·»åŠ åˆ°å‘é‡åº“
        self.vectorstore.add_documents(chunks)

        # æ›´æ–°ç´¢å¼•
        self.papers_index[file_path] = metadata
        self._save_papers_index()

        print(f"âœ… è®ºæ–‡å·²æ·»åŠ : {metadata.title}")
        return metadata

    def add_note(self, content: str, title: str, related_paper: str = None):
        """æ·»åŠ ç¬”è®°"""
        chunks = self.paper_loader.load_text_note(content, title)

        if related_paper:
            for chunk in chunks:
                chunk.metadata["related_paper"] = related_paper

        self.vectorstore.add_documents(chunks)
        print(f"âœ… ç¬”è®°å·²æ·»åŠ : {title}")

    def search(self, query: str, k: int = None, filter_dict: Dict = None) -> List[Document]:
        """
        æ£€ç´¢ç›¸å…³å†…å®¹

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            k: è¿”å›æ•°é‡
            filter_dict: è¿‡æ»¤æ¡ä»¶ï¼Œå¦‚ {"year": 2023}
        """
        k = k or config.RETRIEVER_K

        if filter_dict:
            results = self.vectorstore.similarity_search(
                query, k=k, filter=filter_dict
            )
        else:
            results = self.vectorstore.similarity_search(query, k=k)

        return results

    def search_with_scores(self, query: str, k: int = None) -> List[tuple]:
        """æ£€ç´¢å¹¶è¿”å›ç›¸ä¼¼åº¦åˆ†æ•°"""
        k = k or config.RETRIEVER_K
        return self.vectorstore.similarity_search_with_score(query, k=k)

    def get_retriever(self, k: int = None):
        """è·å–æ£€ç´¢å™¨"""
        k = k or config.RETRIEVER_K
        return self.vectorstore.as_retriever(
            search_kwargs={"k": k}
        )

    def list_papers(self) -> List[PaperMetadata]:
        """åˆ—å‡ºæ‰€æœ‰è®ºæ–‡"""
        return list(self.papers_index.values())

    def get_paper_by_title(self, title: str) -> Optional[PaperMetadata]:
        """æ ¹æ®æ ‡é¢˜æŸ¥æ‰¾è®ºæ–‡"""
        for paper in self.papers_index.values():
            if title.lower() in paper.title.lower():
                return paper
        return None

    def get_stats(self) -> Dict:
        """è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
        papers = self.list_papers()

        # æŒ‰å¹´ä»½ç»Ÿè®¡
        year_counts = {}
        for p in papers:
            if p.year:
                year_counts[p.year] = year_counts.get(p.year, 0) + 1

        # å…³é”®è¯ç»Ÿè®¡
        keyword_counts = {}
        for p in papers:
            for kw in p.keywords:
                keyword_counts[kw] = keyword_counts.get(kw, 0) + 1

        top_keywords = sorted(
            keyword_counts.items(),
            key=lambda x: x[1],
            reverse=True
        )[:10]

        return {
            "total_papers": len(papers),
            "papers_by_year": year_counts,
            "top_keywords": top_keywords,
            "total_chunks": self.vectorstore._collection.count()
        }


# å¯¼å‡ºå•ä¾‹
from .paper_loader import asdict  # éœ€è¦å¯¼å…¥ asdict

knowledge_base = ResearchKnowledgeBase()